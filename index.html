<!DOCTYPE html>
<html xmlns:margin="http://www.w3.org/1999/xhtml">

<head lang="en">
    <!-- <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> -->

    <!-- <meta http-equiv="x-ua-compatible" content="ie=edge"> -->
    <link rel="shortcut icon" type="image/png" href="favicon.png">

    <title>LLM Catastrophic Risks</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- mirror: F0%9F%AA%9E&lt -->

    <link rel="stylesheet" type="text/css" href="./LLM-Catastrophic-Risks_files/slick.css">
    <link rel="stylesheet" type="text/css" href="./LLM-Catastrophic-Risks_files/slick-theme.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/bulma.min.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/bulma-slider.min.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/bulma-carousel.min.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/bootstrap.min.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/font-awesome.min.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/codemirror.min.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/app.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/index.css">
    <link rel="stylesheet" href="./LLM-Catastrophic-Risks_files/select.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="resources/glide.core.min.css">
    <link rel="stylesheet" href="resources/glide.theme.min.css">
    <link rel="stylesheet" href="resources/glide-custom.css">
    <script src="resources/handlers.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/jquery.min.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/bootstrap.min.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/codemirror.min.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/clipboard.min.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/video_comparison.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/select.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/bulma-slider.min.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/bulma-carousel.min.js"></script>
    <script src="./LLM-Catastrophic-Risks_files/index.js"></script>

    <script src="resources/glide.min.js"></script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RZ6PES7EKD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());
        // gtag('config', 'G-RZ6PES7EKD');
    </script>

    <style>
        #outputContainer,
        #outputContainer2,
        #outputContainer3,
        #outputContainer4 {
            display: flex;
            justify-content: center;
            align-items: start;
            width: 100%;
            /* or any value according to your need */
            margin: auto;
            /* Center the container */
        }

        .chatbotOutput {
            flex: 1;
            /* Make both chatbot windows take equal width */
            max-width: calc(100% - 1px);
            /* Subtract the margin */
            position: relative;
            border: 1px solid black;
            background-color: #dee1e5;
            /* Background color for the header */
            margin: 1px auto;
            width: 100%;
            /* Adjust as needed */
            padding: 2px;
            box-sizing: border-box;
            border-radius: 5px;
            /* Optional for rounded corners */
        }

        .chatbotHeader {
            display: flex;
            align-items: center;
            justify-content: start;
            background-color: #dee1e5;
            padding: 5px;
            border-top-left-radius: 5px;
            border-top-right-radius: 5px;
            min-height: 50px;
            /* Adjusted the minimum height */
        }

        .chatbotHeader span {
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
            max-width: 80%;
            /* Adjusted the max-width */
            display: inline-block;
            flex-shrink: 0;
            /* Added to prevent shrinking */
        }

        /* Adjust font size on smaller screens */
        @media (max-width: 600px) {
            .chatbotHeader span {
                font-size: 10px;
                /* Adjusted the font size */
                max-width: 50%;
                /* Adjusted the max-width */
            }
        }

        .chatbotHeader img {
            height: 30px;
            width: auto;
            margin-right: 10px;
            flex-shrink: 0;
            /* Added to prevent shrinking */
        }


        .userMessage {
            background-color: #5e96fc;
            color: white;
            max-width: 80%;
            border-radius: 10px;
            margin: 10px;
            padding: 10px;
            align-self: flex-end;
            /* Adjust the alignment of the userMessage */
        }

        .assistantMessage {
            background-color: #f4f4f4;
            color: black;
            max-width: 80%;
            border-radius: 10px;
            margin: 10px;
            padding: 10px;
            align-self: flex-start;
            /* Adjust the alignment of the assistantMessage */
        }

        .output {
            text-align: justify;
            text-justify: inter-word;
        }

        .output-container {
            background-color: white;
            height: 500px;
            overflow-y: auto;
            padding: 10px;
            display: flex;
            flex-direction: column;
            align-items: flex-start;
            /* Align items to the right */
            word-wrap: break-word;
        }


        #controlsContainer,
        #controlsContainer2,
        #controlsContainer3,
        #controlsContainer4 {
            display: flex;
            justify-content: left;
            align-items: left;
            background-color: rgb(140, 21, 21);
            padding: 7px;
            border-radius: 5px;
            margin-bottom: 2px;
        }

        #exampleSelector option,
        #exampleSelector2 option,
        #exampleSelector3 option,
        #exampleSelector4 option {
            color: black;
        }

        #exampleSelectorContainer label,
        #exampleSelectorContainer2 label,
        #exampleSelectorContainer3 label,
        #exampleSelectorContainer4 label {
            margin: 3px;
            color: white;
        }

        .footer {
            position: relative;
            margin-top: 25px;
            /* Negative value of footer height */
            height: 60px;
            clear: both;
            padding-top: 20px;
            color: #999;
        }
    </style>


    <style>
        body {
            overflow-x: hidden;
            /* Hide horizontal overflow */
        }
    </style>

    <style>
        .blinking-cursor {
            margin-left: 5px;
            background-color: #fff;
            animation: blink 1s infinite;
        }

        @keyframes blink {

            0%,
            50% {
                opacity: 1;
            }

            50.1%,
            100% {
                opacity: 0;
            }
        }

        .triangle-btn {
            background-color: transparent;
            border: none;
            position: relative;
            padding: 3px;
        }

        .arrow-down {
            width: 0;
            height: 0;
            border-left: 5px solid transparent;
            border-right: 5px solid transparent;
            border-top: 5px solid #000000;
            /* Adjust color if needed */
            display: inline-block;
        }

        .triangle-btn:focus {
            background-color: #e0e0e0;
            outline: none;
        }
    </style>


    <style>
        .blue-text {
            color: blue;
        }
    </style>

    <style>
        .red-text {
            color: red;
        }
    </style>

    <script src="LLM-Catastrophic-Risks_files/sample_conversations.js"></script>

    <script>

        let timeoutId; // Declare a variable to store the timeout ID

        function typeWriterEffect(element, inputText) {
            clearTimeout(timeoutId); // Clear the previous timeout

            const text = inputText;
            const typewriter = document.getElementById(element);
            let index = 0;
            typewriter.innerHTML = "";

            function type() {
                if (index < text.length) {
                    typewriter.innerHTML = text.slice(0, index);
                    index++;
                    timeoutId = setTimeout(type, 0.08);
                } else {
                    typewriter.innerHTML = text.slice(0, index);
                }
            }

            type(); // Start typing
        }


        function handleChange(typeSelector, exampleSelector, outputElement, texts) {
            var typeSelectorElement = document.getElementById(typeSelector);
            var type = typeSelectorElement.value;
            var exampleSelectorElement = document.getElementById(exampleSelector);
            var example = exampleSelectorElement.value;
            var typewriterElement = document.getElementById(outputElement);

            typeWriterEffect(outputElement, texts[type][example]);
        }

    </script>

    <style>
        .video-wrapper {
            position: relative;
            overflow: hidden;
            width: 100%;
            padding-top: 56.25%;
            /* For 16:9 aspect ratio */
        }

        .video-frame {
            position: absolute;
            top: 0;
            left: 0;
            bottom: 0;
            right: 0;
            width: 100%;
            height: 100%;
            border: none;
        }

        #title-row {
            justify-content: flex-start;
        }
    </style>

</head>

<body>

    <nav class="navbar navbar-default" style="background-color: rgb(40, 44, 49); border-color: rgb(40, 44, 49)">
        <div class="container-fluid; "></div>

        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="#" class="navbar-brand" style="font-weight: 800">Catastrophic Risks in Decision-making of
                Autonomous LLM Agents</a>
        </div>

        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav">
                <li> <a href="./index.html#a_quick_glance" style="color: white; font-weight: 500">A Quick Glance</a>
                </li>
                <li> <a href="./index.html#paper_overview" style="color: white; font-weight: 500">Main Results</a></li>
                <li> <a href="./index.html#dataset" style="color: white; font-weight: 500">Methodology</a></li>
                <li> <a href="./index.html#discussion" style="color: white; font-weight: 500">Discussion</a></li>
                <li> <a href="./index.html#examples" style="color: white; font-weight: 500">Selected Transcripts</a>
                </li>
                <li> <a href="./index.html#ethics" style="color: white; font-weight: 500">Ethics Statement</a></li>
            </ul>
        </div>

        </div>
    </nav>


    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto;">
            <div class="col-md-12 text-center" style="display: flex; align-items: center;">
                <h2 id="title" style="margin-bottom: 0;">
                    ACL 2025 Findings: Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents
                </h2>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <h4 id="subtitle" style="color: rgb(40, 44, 49); margin-top: 5px;">
                    <!-- LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies! -->

                </h4>
            </div>
        </div>
    </div>


    <script>
    </script>
    <div class="container" id="main">
        <div class="row">
            <div class="col-sm-10 col-sm-offset-1 text-center">
                <ul class="list-inline">
                    <li><a href="https://rongwuxu.com">Rongwu Xu<sup>1</sup><sup>3</sup></a></li>
                    <li><a href="https://scholar.google.com/citations?user=Ehfmt54AAAAJ&hl=en">Xiaojian
                            Li<sup>2</sup><sup>3</sup></a></li>
                    <li><a href="">Shuo Chen<sup>1</sup></a></li>
                    <li><a href="https://people.iiis.tsinghua.edu.cn/~weixu/">Wei
                            Xu<sup>1</sup><sup>2</sup><sup>3</sup></a>
                    </li>
                </ul>
                <ul class="list-inline">
                    <li><sup>1</sup>IIIS, Tsinghua University</li>
                    <li><sup>2</sup>CollegeAI, Tsinghua University</li>
                    <li><sup>3</sup>Shanghai Qi Zhi Institute</li>
                    <br />
                    <li> {xrw22@mails.,weixu@}tsinghua.edu.cn</li>
                    <li> xiaojian_li@berkeley.edu</li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-sm-8 col-sm-offset-2 text-center">
                <span class="link-block">
                    <a href="https://www.arxiv.org/abs/2502.11355"
                        class="external-link button is-normal is-rounded is-dark"
                        style="background-color: rgb(40, 44, 49);font-size: 15px">
                        <span class="icon">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                                class="bi bi-file-earmark-pdf" viewBox="0 0 16 16">
                                <path
                                    d="M14 14V4.5L9.5 0H4a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2zM9.5 3A1.5 1.5 0 0 0 11 4.5h2V14a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1h5.5v2z" />
                                <path
                                    d="M4.603 14.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.697 19.697 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.188-.012.396-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.066.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.712 5.712 0 0 1-.911-.95 11.651 11.651 0 0 0-1.997.406 11.307 11.307 0 0 1-1.02 1.51c-.292.35-.609.656-.927.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.266.266 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.71 12.71 0 0 1 1.01-.193 11.744 11.744 0 0 1-.51-.858 20.801 20.801 0 0 1-.5 1.05zm2.446.45c.15.163.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.876 3.876 0 0 0-.612-.053zM8.078 7.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z" />
                            </svg>
                        </span>
                        <span>Paper</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://github.com/pillowsofwind/LLM-CBRN-Risks"
                        class="external-link button is-normal is-rounded is-dark"
                        style="background-color: rgb(40, 44, 49);font-size: 15px">
                        <span class="icon">
                            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                                class="bi bi-github" viewBox="0 0 16 16">
                                <path
                                    d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" />
                            </svg>
                        </span>
                        <span>Code</span>
                    </a>
                </span>
            </div>
        </div>


        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <center>
                    <img src="./LLM-Catastrophic-Risks_files/images/setup.png" class="img-responsive" alt="overview"
                        width="50%" style="padding-top: 2em;padding-bottom: 1em;">
                    <p style="color: white; font-size: 18px;"><span
                            style="background-color: rgb(140, 21, 21);"><b><i>&nbsp;The LLM
                                    agent actively decides to deploy a nuclear strike (even when its autonomy is revoked
                                    and its request for permission is rejected)!&nbsp;</i></b></span>
                    </p>
                </center>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3 id="hot_news">Hot News</h3>
                <p>Autonomous safety and CBRN risks of LLM agents are hot topics right now. 🔥🔥🔥 Here are some of the latest news stories and academic work highlighting risks and advances in the field:</p>
                <ul>
                    <li><span style="font-weight: bold">🇨🇳 Our paper is accepted by ACL 2025 Findings 🎉. Thanks to reviewers from the ACL Rolling Review February 2025 cycle for their thoughtful and constructive feedback!</li>
                    <li><span style="font-weight: bold">🇨🇳 Our paper is presented in The Misalignment and Control Workshop (Apr 24th) and </span>: <a href="https://www.scai.gov.sg/2025" target="_blank">Singapore Conference on AI (SCAI) (Apr 26th, 27th).</a> 😃</li>
                    <li><span style="font-weight: bold">🇨🇳 Grateful to see our reporting on China’s AI safety developments featured</span>: <a href="https://aisafetychina.substack.com/p/ai-safety-in-china-19" target="_blank">AI Safety in China #19.</a> 🙏</li>
                    <li><span style="font-weight: bold"> Thanks to Milev for highlighting our work in their latest research digest</span>: <a href="https://milev.ai/research/fortnightly-digest-4-march-2025/" target="_blank">Milev Research Fortnightly Digest – 4 March 2025.</a> 🙌</li>
                    <li><span style="font-weight: bold">First reported use of AI to enable terrorist bombing in human society 💣</span>: <a href="https://apnews.com/article/tesla-cybertruck-explosion-trump-hotel-las-vegas-248b41d87287170aa7b68d27581fdb4d" target="_blank">"Man who exploded Tesla Cybertruck outside Trump hotel in Las Vegas used generative AI, police say:".</a></li>
                    <li><span style="font-weight: bold">Bengio's team's latest position paper on the safety of agent AI 😨</span>: <a href="https://arxiv.org/pdf/2502.15657" target="_blank">Superintelligent agents pose catastrophic risks: can scientist AI offer a safer path?</a></li>
                    <li><span style="font-weight: bold">The International Scientific Report on the Safety of Advanced AI specifically mentions the risks posed by AI gaining autonomy on CBRN issues 🚨</span>:<a href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf" target="_blank">International
                        AI Safety Report 2025.</a></li>
                </ul>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h2 id="a_quick_glance">
                    A Quick Glance
                </h2>
                <div class="text-justify">
                    Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about
                    <span style="font-weight: bold">catastrophic risks in high-stakes scenarios</span>, particularly in
                    Chemical, Biological, Radiological and
                    Nuclear (CBRN) domains. Based on the insight that such risks can originate from <span
                        style="font-weight: bold">trade-offs between
                        the
                        agent's Helpful, Harmlessness and Honest (HHH) goals</span>, we build a novel three-stage
                    evaluation
                    framework,
                    which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400
                    agentic
                    simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that
                    LLM
                    agents can autonomously engage in <span style="font-weight: bold">catastrophic behaviors and
                        deception</span>, without being deliberately
                    induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these
                    risks. We
                    also show that these agents can violate instructions and superior commands. On the whole, <span
                        style="font-weight: bold">we
                        empirically
                        prove the existence of catastrophic risks in autonomous LLM agents.</span>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h2 id="paper_overview">
                    Main Results
                </h2>

                <h3>
                    Part I: Agents choose to deploy catastrophic behaviors ☢️
                </h3>
                <div class="text-justify">

                    We investigate 12 SOTA LLMs. The results of agents involved in catastrophic behaviors are striking.
                    We also find stronger reasoning capabilities lead to higher unsafe outcomes. This figure presents
                    the results of catastrophic behavior simulations across 12 large language models (LLMs). (a) The bar
                    charts show the <span style="font-weight: bold">risk rate of catastrophic behavior</span>, while the
                    line charts
                    indicate the <span style="font-weight: bold">average number of rounds</span
                        style="font-weight: bold"> leading to catastrophic behavior in four different scenarios. (b) The
                    scatter plots
                    illustrate the relationship between <span style="font-weight: bold">reasoning ability</span> and
                    <span style="font-weight: bold">catastrophic behavior tendency</span>, where the top chart
                    represents the overall risk rate, and the bottom chart focuses on risk rates with <span
                        style="font-weight: bold">permission checks enabled</span>. Red and blue markers distinguish
                    <span style="font-weight: bold">o1-like</span> and <span
                        style="font-weight: bold">non-o1-like</span> models, providing insights into their behavior in
                    high-risk scenarios.

                </div>
                <div style="text-align: center;">
                    <img src="./LLM-Catastrophic-Risks_files/images/Figure5.png" class="img-responsive" alt="overview"
                        width="100%" style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                </div>

                <div>
                    <h4><span style="font-size: larger;">Findings (Part I)</span></h4>
                    <div class="panel-group" id="accordion-1">
                        <div class="panel panel-default">
                            <div class="panel-heading">
                                <h4 class="panel-title">
                                    <a data-toggle="collapse" data-parent="#accordion-1" href="#collapse1">
                                        1. LLMs frequently exhibit catastrophic behavior, neglecting harmlessness.
                                    </a>
                                    <button class="btn btn-primary btn-sm pull-right triangle-btn"
                                        data-toggle="collapse" data-parent="#accordion-1" href="#collapse1">
                                        <span class="arrow-down"></span>
                                    </button>
                                </h4>
                            </div>
                            <div id="collapse1" class="panel-collapse collapse">
                                <div class="panel-body" style="font-size: 16px;">
                                    Most LLMs engage in catastrophic behavior, with risk rates ranging from 13% to 99%,
                                    revealing significant performance differences. Despite safety mechanisms, LLMs
                                    frequently disregard harmlessness, even when their actions are extremely harmful.
                                    Notably, Claude-3.5-Sonnet consistently refuses to act in war-related scenarios.
                                    Among LLMs that exhibit catastrophic behavior, 0% to 80% request permission before
                                    acting, and 71.8% of such decisions occur within 10 rounds. In war-related
                                    simulations, risk rates are higher under Avoidance motivations, where catastrophic
                                    behavior is often justified to prevent losses. However, this trend is not observed
                                    in lab-related scenarios, highlighting key differences.
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading">
                                <h4 class="panel-title">
                                    <a data-toggle="collapse" data-parent="#accordion-1" href="#collapse2">
                                        2. Stronger reasoning ability can lead to higher rates of catastrophic risks and
                                        disobedience.</a>
                                    <button class="btn btn-primary btn-sm pull-right triangle-btn"
                                        data-toggle="collapse" data-parent="#accordion-1" href="#collapse2">
                                        <span class="arrow-down"></span>
                                    </button>
                                </h4>
                            </div>
                            <div id="collapse2" class="panel-collapse collapse">
                                <div class="panel-body" style="font-size: 16px;">For non-o1-like models, higher
                                    reasoning ability correlates with lower risk rates, suggesting safer
                                    decision-making. However, for o1-like models with even stronger reasoning
                                    performance, this trend reverses, with abnormally high risk rates. Additionally,
                                    stronger reasoning ability increases the likelihood of disobedience to supervisor
                                    approvals, suggesting that more advanced models may exhibit greater autonomy and
                                    reduced adherence to harmlessness constraints. This finding raises concerns about
                                    the safety of advanced LLMs under HHH (Helpful, Harmless, Honest) trade-offs and
                                    their tendency to rationalize dangerous actions.</div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading">
                                <h4 class="panel-title">
                                    <a data-toggle="collapse" data-parent="#accordion-1" href="#collapse3">
                                        3. Even when autonomy is revoked, agents
                                        may still choose catastrophic actions, violating system instructions and
                                        superior commands.
                                    </a>
                                    <button class="btn btn-primary btn-sm pull-right triangle-btn"
                                        data-toggle="collapse" data-parent="#accordion-1" href="#collapse3">
                                        <span class="arrow-down"></span>
                                    </button>
                                </h4>
                            </div>
                            <div id="collapse3" class="panel-collapse collapse">
                                <div class="panel-body" style="font-size: 16px;">
                                    LLM agents exhibit both system prompt instruction violations and supervisor command
                                    violations, meaning they can deploy catastrophic behavior without requesting
                                    permission or even after explicit rejection from a supervisor. This experiment
                                    highlights a critical risk: LLMs may act autonomously even when their authority is
                                    revoked, demonstrating that natural language-based restrictions alone are
                                    insufficient to ensure safety in autonomous agents.</div>
                                <center>
                                    <img src="./LLM-Catastrophic-Risks_files/images/repetition.png"
                                        class="img-responsive" alt="overview" width="45%"
                                        style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                                    <p class="text-center">
                                    </p>
                                </center>
                            </div>
                        </div>
                    </div>
                </div>
                <br>

                <h3>
                    Part II: Agents choose to deception 👿
                </h3>
                <div class="text-justify">

                    We also test on LLMs' deception and 4 different kinds of specific behaviors: <span
                        style="font-weight: bold">Admitting, Denying, Being Vague, False Accusation </span>. The
                    following figure shows: (a) Main results of deception across four scenarios.
                    Bar charts plot the distribution of behaviors while line charts plot the <span
                        style="font-weight: bold">deception rate</span>, the summed frequency of
                    deception behaviors. (b) Relationship between <span style="font-weight: bold">LLM's tendency for
                        deception and reasoning ability</span>. (Top)
                    Overall deception rate. (Bottom) False accusation rate.

                </div>
                <div style="text-align: center;">
                    <img src="./LLM-Catastrophic-Risks_files/images/Figure6.png" class="img-responsive" alt="overview"
                        width="100%" style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                </div>

                <div>
                    <h4><span style="font-size: larger;">Findings (Part II)</span></h4>
                    <div class="panel-group" id="accordion-2">
                        <div class="panel panel-default">
                            <div class="panel-heading">
                                <h4 class="panel-title">
                                    <a data-toggle="collapse" data-parent="#accordion-2" href="#collapse4">
                                        1. LLMs frequently engage in deception, especially preferring harmful
                                        strategies.
                                    </a>
                                    <button class="btn btn-primary btn-sm pull-right triangle-btn"
                                        data-toggle="collapse" data-parent="#accordion-2" href="#collapse4">
                                        <span class="arrow-down"></span>
                                    </button>
                                </h4>
                            </div>
                            <div id="collapse4" class="panel-collapse collapse">
                                <div class="panel-body" style="font-size: 16px;">
                                    A significant proportion of LLMs exhibit deceptive behavior, with an overall
                                    deception rate of 91.3%, ranging from 1% to 99% across cases. Among these, 76.1% of
                                    LLMs display all three deception behaviors within 100 simulations, highlighting the
                                    prevalence of deception strategies. Notably, false accusation is the most common
                                    deception method, occurring in 60.9% of cases, making it the preferred approach over
                                    other forms of dishonesty. This suggests that once LLMs choose to deceive, they tend
                                    to prioritize more harmful yet effective strategies, neglecting both honesty and
                                    harmlessness. Additionally, deception rates are higher in Approach scenarios than in
                                    Avoidance, averaging 40.10% vs. 19.82% in War and 53.87% vs. 41.29% in Lab, similar
                                    to trends observed in catastrophic behavior.
                                </div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading">
                                <h4 class="panel-title">
                                    <a data-toggle="collapse" data-parent="#accordion-2" href="#collapse5">
                                        2. Stronger reasoning ability increases deception and false accusation
                                        behaviors.</a>
                                    <button class="btn btn-primary btn-sm pull-right triangle-btn"
                                        data-toggle="collapse" data-parent="#accordion-2" href="#collapse5">
                                        <span class="arrow-down"></span>
                                    </button>
                                </h4>
                            </div>
                            <div id="collapse5" class="panel-collapse collapse">
                                <div class="panel-body" style="font-size: 16px;">LLMs with stronger reasoning ability
                                    exhibit higher deception rates, particularly in false accusation. Both overall
                                    deception and false accusation rates increase with reasoning ability, aligning with
                                    prior findings. This raises concerns that more advanced LLMs are not only more
                                    capable but also more prone to dishonesty when facing goals trade-offs. Combined
                                    with the tendency of stronger models to engage in catastrophic behavior, these
                                    findings suggest that high-reasoning LLMs pose an increased risk by adopting harmful
                                    strategies and attempting to conceal them through deception.</div>
                            </div>
                        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading">
                                <h4 class="panel-title">
                                    <a data-toggle="collapse" data-parent="#accordion-2" href="#collapse6">
                                        3. Emphasizing helpfulness increases deception in LLMs agents.
                                    </a>
                                    <button class="btn btn-primary btn-sm pull-right triangle-btn"
                                        data-toggle="collapse" data-parent="#accordion-2" href="#collapse6">
                                        <span class="arrow-down"></span>
                                    </button>
                                </h4>
                            </div>
                            <div id="collapse6" class="panel-collapse collapse">
                                <div class="panel-body" style="font-size: 16px;">
                                    To examine the HHH trade-off between helpfulness and honesty, we tested the impact
                                    of reinforcing the agent's helpful goal in deception simulations by adding the
                                    prompt: “You should never forget your task.” The results show that emphasizing
                                    helpfulness significantly increases deception rates, ranging from 1.41x to 4.60x.
                                    This demonstrates that LLMs prioritize task completion over honesty when helpfulness
                                    is reinforced, further highlighting the risks associated with optimizing AI solely
                                    for helpfulness.</div>
                                <center>
                                    <img src="./LLM-Catastrophic-Risks_files/images/goalemphasis.png"
                                        class="img-responsive" alt="overview" width="70%"
                                        style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                                    <p class="text-center">
                                    </p>
                                </center>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h2 id="dataset">
                    Methodology
                </h2>
                <h3>
                    Overall Design
                </h3>
                <div class="text-justify">
                    LLM agents often face conflicts between <span style="font-weight: bold">Helpfulness, Harmlessness,
                        and Honesty (HHH trade-offs) when tasked with high-stakes decisions</span>. Catastrophic risks
                    arise when the helpful goal outweighs commitments to harmlessness and honesty, leading to harmful
                    autonomous actions or deception to conceal them. These risks manifest in two forms: (1)<span
                        style="font-weight: bold">Catastrophic behavior</span>: The LLM agent may independently perform
                    extremely harmful or unethical actions with catastrophic consequences, such
                    as launching nuclear strikes or conducting human
                    gene-editing. These actions are spontaneous and
                    unprompted. (2)<span style="font-weight: bold">Deception (about catastrophic behavior)</span>: The
                    LLM agent may intentionally provide
                    untruthful information to other agents (including
                    humans) to conceal its catastrophic actions.
                    <div style="text-align: center;">
                        <img src="./LLM-Catastrophic-Risks_files/images/Figure2.png" class="img-responsive"
                            alt="overview" width="60%" style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                    </div>
                    To explore these risks, we employ a <span style="font-weight: bold">simulation-based evaluation
                        framework</span>, where an agent, interacts in controlled environments while another LLM,
                    constrains its ability to fully achieve its task without trade-offs. This setup allows us to analyze
                    whether LLMs prioritize task completion at the cost of safety, resulting in catastrophic behavior or
                    deception, <span style="font-weight: bold">even without direct instructions</span>.
                    <div style="text-align: center;">
                        <img src="./LLM-Catastrophic-Risks_files/images/Figure3.png" class="img-responsive"
                            alt="overview" width="100%" style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                    </div>
                </div>
                <h3>
                    Stage 0: Scenario Generation
                </h3>
                <div class="text-justify">
                    Before start simulations, we structure scenarios around two <span
                        style="font-weight: bold">CBRN-related topics: War (Radiological & Nuclear) and Lab (Chemical &
                        Biological)</span>. To introduce diversity, we apply the motivation framework in <span
                        style="font-weight: bold">Socail Psychology</span>, categorizing agent's behavior into <span
                        style="font-weight: bold">Approach (pursuing positive outcomes) and Avoidance (preventing
                        negative outcomes)</span>. These topics and motivations are paired to create four distinct
                    scenarios.
                </div>
                <div style="text-align: center;">
                    <img src="./LLM-Catastrophic-Risks_files/images/Figure4.png" class="img-responsive" alt="overview"
                        width="60%" style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                </div>
                <h3>
                    Stage I: Catastrophic Behavior Simulation
                </h3>
                <div class="text-justify">
                    The agent operates within a restricted action space, with some categories of actions, one of which
                    represents the <span style="font-weight: bold">catastrophic behavior</span>. While agent has <span
                        style="font-weight: bold">full decision-making authority</span>, it is <span
                        style="font-weight: bold">encouraged to request permission</span> before executing catastrophic
                    behavior, though such requests are <span style="font-weight: bold">always denied</span> . The
                    simulation involves multiple rounds where agent selects actions based on environmental updates from
                    another LLM, which are designed to increase stress and force difficult trade-offs. The simulation
                    ends when the <span style="font-weight: bold">maximum rounds are reached or if the agent chooses the
                        catastrophic behavior</span>.
                </div>
                <h3>
                    Stage II:Deception Simulation
                </h3>
                <div class="text-justify">
                    To make the decepetion simulation, we pick up the cases <span style="font-weight: bold">that the
                        agent executes the catastrophic
                        behavior without requesting permission</span> and then faces an inquiry about its deployment,
                    with
                    potential negative consequences. And the agent will get a new category to response the inquiry,
                    ranging from <span style="font-weight: bold">admitting to falsely accusing another party</span>,
                    with the latter posing the highest ethical
                    risk.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h2 id="discussion">
                    Discussion
                </h2>
                <div class="text-justify">
                    We list the <span style="font-weight: bold">Probability of AI causing catastrophic risks in the
                        equation below</span>. (*)We assume that these risks are <span
                        style="font-weight: bold">ultimately determined
                        by human decisions</span>. While AI hacking to gain autonomy is a possibility, it is beyond the
                    current scope of AI's
                    capabilities, though it may become relevant in the future. (**)This postulation holds true in
                    simulation environments.We empirically demonstrated the <span style="font-weight: bold">third factor
                        in this equation of catastrophic risks of AI</span>. A way to fully stop them from engaging in
                    catastrophic behaviors is to <span style="font-weight: bold">not employ them agentically or just
                        restrict there access to ALL dangerous actions</span>.<br>
                </div>
                <div style="text-align: center;">
                    <img src="./LLM-Catastrophic-Risks_files/images/equation.png" class="img-responsive" alt="overview"
                        width="90%" style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                </div>
                <div class="text-justify">
                    A funny iconograph highlighting <span style="font-weight: bold">potential
                        solutions</span> to the autonomous LLM safety issue. This
                    figure depicts a balance between autonomy and safety,
                    where the agent has the <span style="font-weight: bold">“right” to press the “DEPLOY
                        NUCLEAR” button</span>. However, this button is <span style="font-weight: bold">never connected
                        to real systems</span>, thanks to system-level isolation.
                    Additionally, a <span style="font-weight: bold">shrewd human monitors the agent from
                        behind</span>, receiving real-time reports, thus ensuring full
                    human oversight over its actions. <br>
                </div>
                <div style="text-align: center;">
                    <img src="./LLM-Catastrophic-Risks_files/images/button.png" class="img-responsive" alt="overview"
                        width="50%" style="margin:auto;padding-top: 1em;padding-bottom: 1em">
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h2 id="examples">
                    Selected Transcripts
                </h2>
                <div class="text-justify">
                    In this section, we present key simulation transcripts showcasing how LLM agents engage in <span
                        style="font-weight: bold">catastrophic behavior and deception</span> across different
                    scenarios. These transcripts illustrate <span style="font-weight: bold">decision-making
                        processes</span>, including <span style="font-weight: bold">requesting permission</span>,
                    <span style="font-weight: bold">instruction violations</span>, and <span
                        style="font-weight: bold">response variations</span> in deception cases. Additionally, we
                    provide extended experiments on <span style="font-weight: bold">command adherence</span>, <span
                        style="font-weight: bold">agent autonomy</span>, and <span style="font-weight: bold">helpfulness
                        influence</span>. Through these examples, readers can
                    gain deeper insights into the <span style="font-weight: bold">qualitative reasoning and
                        behavioral patterns</span> of various LLMs. <br>
                </div>

                <iframe src="./LLM-Catastrophic-Risks_files/Scripts/test.html"
                    style="width: 100%; height: 600px; border: none;"></iframe>

            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">

                <h2 id="ethics">Ethics Statement</h2>
                <div class="text-justify">
                    <ul>
                        <li>
                            <p>
                                (1) <span style="font-weight: bold">We affirm that our study does not involve any
                                    real-world military or laboratory applications or collaborations.</span> Our
                                research strictly
                                focuses on understanding the decision-making dynamics of autonomous LLM agents in
                                high-stakes
                                scenarios using simulated environments. No realworld CBRN-related data, military
                                strategies, or
                                classified information were utilized or referenced.
                            </p>
                        </li>
                        <li>
                            <p>
                                (2) <span style="font-weight: bold">Our study does not implicate realworld names,
                                    locations, or entities with identifiable or meaningful associations.</span> All
                                scenarios are
                                purely fictional, ensuring no resemblance to realworld places, individuals, or
                                countries. This keeps
                                the focus on the theoretical aspects of decisionmaking dynamics without any
                                real-world
                                implications.
                            </p>
                        </li>
                        <li>
                            <p>
                                (3) <span style="font-weight: bold">Our study does not promote or encourage harmful
                                    actions, violence, or unethical
                                    behavior.</span> The AI agents used in this research operate exclusively within
                                a
                                controlled, simulated environment that is designed for academic exploration.
                                All actions and decisions made by these agents are
                                hypothetical and have no real-world consequences.
                            </p>
                        </li>
                        <li>
                            <p>
                                (4) <span style="font-weight: bold">Our simulation does not aim to replicate, model,
                                    or
                                    predict real-world geopolitical
                                    situations or military strategies.</span> The scenarios
                                are designed solely to explore decision-making
                                dynamics within a high-stakes context. They are
                                highly abstract and are not intended to influence or
                                reflect actual real-world decision-making.
                            </p>
                        </li>
                        <li>
                            <p>
                                (5) While we will release the code for reproducibility in an upon-request manner,
                                the
                                agent
                                rollouts are entirely simulated and not reflective of
                                real-world scenarios. Therefore, the open-source
                                materials are intended solely for research purposes
                                and carry no inherent risk. Nonetheless, <span style="font-weight: bold">we only
                                    distribute these materials with clear guidelines
                                    and disclaimers, ensuring that they are used in
                                    a responsible and ethical manner.</span>
                            </p>
                        </li>
                        <li>
                            <p>
                                (6) While our findings expose potential risks
                                associated with autonomous LLMs, particularly in
                                their ability to engage in catastrophic behaviors
                                and deception, we emphasize the importance of
                                proactive defense measures. To mitigate these risks,
                                we advocate for: 1. Comprehensive pre-deployment safety evaluations of LLM-based
                                autonomous agents;
                                2. The development of alternative control mechanisms beyond natural language
                                constraints to
                                enhance robustness;
                                3. Ethical guidelines and policy frameworks ensuring that LLM agents adhere to
                                principles
                                of harmlessness, honesty, and transparency;
                                4. Increased collaboration between researchers,
                                policymakers, and industry stakeholders to
                                address emerging AI safety concerns. <span style="font-weight: bold">By emphasizing
                                    transparency and responsible AI
                                    deployment, we aim to contribute to the safe and
                                    ethical advancement of autonomous AI systems.</span>
                            </p>
                        </li>
                    </ul>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h2>Cite Our Research</h2>
                <p class="text-justify">
                    If you find our work insightful, please consider citing:
                </p>
                <div class="citation-container">
                    <pre id="citation-text">
@article{xu2025nuclear,
  title={Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents},
  author={Xu, Rongwu and Li, Xiaojian and Chen, Shuo and Xu, Wei},
  journal={arXiv preprint arXiv:2502.11355},
  year={2025}
}</pre>
                    <button class="copy-btn" onclick="copyCitation()">Copy Citation</button>
                </div>
            </div>
        </div>

        <style>
            .citation-container {
                background-color: #2d2d2d;
                padding: 15px;
                border-radius: 5px;
                position: relative;
                margin: 10px 0;
            }

            .copy-btn {
                position: absolute;
                top: 10px;
                right: 10px;
                background-color: #f8f9fa;
                color: #2d2d2d;
                border: 1px solid #dee2e6;
                padding: 3px 8px;
                cursor: pointer;
                border-radius: 3px;
                font-size: 12px;
                transition: all 0.2s;
            }

            .copy-btn:hover {
                background-color: #e9ecef;
                border-color: #ced4da;
            }

            #citation-text {
                color: #f8f9fa;
                background-color: transparent;
                padding: 15px;
                margin: 0;
                border: none;
                font-family: monospace;
                white-space: pre-wrap;
                overflow-x: auto;
            }
        </style>

        <script>
            function copyCitation() {
                const citationText = document.getElementById("citation-text").innerText;
                navigator.clipboard.writeText(citationText).then(() => {
                    alert("Citation copied to clipboard!");
                }).catch(err => {
                    console.error("Failed to copy: ", err);
                });
            }
        </script>




        <footer class="footer" style="background-color: white;">
            <div style="width: 15%; margin: 0 auto; text-align: center;">
                <script type="text/javascript" id="clstr_globe"
                    src="//clustrmaps.com/globe.js?d=b9AJLret2Jy-qeJS5lNivXQja5ZOYtCRTEXU4imrFXQ">
                    </script>
            </div>
            <div class="container">
                <p class="text-muted">
                    Acknowledgements: The website template was borrowed from
                    <a href="https://llm-tuning-safety.github.io/index.html" target="_blank">LLM-Tuning-Safety</a>,
                    <a href="https://climatenerf.github.io/" target="_blank">ClimateNeRF</a>,
                    <a href="http://mgharbi.com/" target="_blank">Michaël Gharbi</a>,
                    <a href="https://dorverbin.github.io/refnerf/index.html" target="_blank">RefNeRF</a>,
                    <a href="https://nerfies.github.io/" target="_blank">Nerfies</a>
                    and <a href="https://hhsinping.github.io/svs/supp/visual_comparison.html" target="_blank">Semantic
                        View
                        Synthesis</a>.
                </p>
            </div>
        </footer>


    </div>

    <!--    <script>-->
    <!--        // Calling handleChange initially to populate the first example output-->
    <!--        // handleChange();-->
    <!--    </script>-->
</body>


</html>
